{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import spacy\n",
    "from datetime import datetime as dt\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_Tweets_df = pd.read_csv('/home/is/shuntaro-o/dev/disaster_analysis_Twitter/data/Hurricane/merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_df = pd.read_csv('/home/is/shuntaro-o/dev/disaster_analysis_Twitter/data/Hurricane/estimated_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AuthorID_list = merged_Tweets_df['AuthorID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_Tweets_df[\"CreateTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list =  merged_Tweets_df['CreateTime'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = []\n",
    "m_list = []\n",
    "d_list = []\n",
    "ymd_list = []\n",
    "for time in time_list:\n",
    "    y_list.append(int(time[0:4]))\n",
    "    m_list.append(int(time[5:7]))\n",
    "    d_list.append(int(time[8:10]))\n",
    "    ymd_list.append(time[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ユーザーごと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sequence = merged_Tweets_df[merged_Tweets_df['AuthorID']==AuthorID_list[10]]\n",
    "m = folium.Map(location=[35.6, 139.7], zoom_start=7.0)\n",
    "for index, r in user_sequence.iterrows():\n",
    "    folium.Marker([r.UserPlase_latitude,r.UserPlase_longitude]).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_True = estimated_df[estimated_df[\"code\"]==estimated_df[\"geo_code\"]]\n",
    "estimated_False = estimated_df[estimated_df[\"code\"]!=estimated_df[\"geo_code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[35.6, 139.7], zoom_start=7.0)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "for index, r in estimated_True.iterrows():\n",
    "    folium.Marker([r.UserPlase_latitude,r.UserPlase_longitude]).add_to(marker_cluster)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[35.6, 139.7], zoom_start=7.0)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "for index, r in estimated_False.iterrows():\n",
    "    folium.Marker([r.UserPlase_latitude,r.UserPlase_longitude]).add_to(marker_cluster)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# モデルのロード\n",
    "nlp = spacy.load(\"ja_core_news_md\")\n",
    "# 解析対象のテキストa\n",
    "input_text = \"8月に東京に行く\"\n",
    "# モデルに解析対象のテキストを渡す\n",
    "doc = nlp(input_text)\n",
    "# 固有表現を抽出\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent.start_char, ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_True = estimated_True.Text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_in_True=0\n",
    "for text in sentences_True:\n",
    "    Flag=0\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_==\"GPE\":\n",
    "            Flag=1\n",
    "        if Flag==1:\n",
    "            counter_in_True+=1\n",
    "counter_in_True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_False = estimated_False.Text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_in_False=0\n",
    "for text in sentences_True:\n",
    "    Flag=0\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_==\"GPE\":\n",
    "            Flag=1\n",
    "        if Flag==1:\n",
    "            counter_in_False+=1\n",
    "counter_in_False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_in_False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "時間分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "１dayごと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_line_df = pd.read_csv(\"/home/is/shuntaro-o/dev/disaster_analysis_Twitter/data/Hurricane/split_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_times = time_line_df.ymd.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_times.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_nums_per_day = []\n",
    "for i in compare_times:\n",
    "    tweet_nums_per_day.append(len(time_line_df[time_line_df['ymd']==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_for_xlabel = []\n",
    "for i in compare_times:\n",
    "    hour_for_xlabel.append(str(i)[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel = []\n",
    "for i in compare_times:\n",
    "    xlabel.append('{}/{}'.format(str(i)[4:6],str(i)[6:8]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16, 12)\n",
    "plt.xticks(rotation=30)\n",
    "plt.tick_params(direction = \"inout\", length = 5, colors = \"white\")\n",
    "plt.xticks(np.arange(0, 123, 10))\n",
    "plt.plot(xlabel,tweet_nums_per_day)\n",
    "plt.xlabel(\"time\",color=\"white\")\n",
    "plt.ylabel(\"sum_tweets\",color=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1hourごと\n",
    "hagibis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_times_hour = time_line_df.ymdh.unique()\n",
    "compare_times_hour.sort()\n",
    "compare_times_hour = compare_times_hour[2328:2592]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_hour = []\n",
    "for i in compare_times_hour:\n",
    "    xlabel_hour.append('{}/{}/{}'.format(str(i)[4:6],str(i)[6:8],str(i)[8:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_nums_per_hour = []\n",
    "for i in compare_times_hour:\n",
    "    tweet_nums_per_hour.append(len(time_line_df[time_line_df['ymdh']==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import color\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 12)\n",
    "plt.xticks(rotation=30)\n",
    "plt.tick_params(direction = \"inout\", length = 5, colors = \"white\")\n",
    "plt.xticks(np.arange(0, 264, 24))\n",
    "plt.plot(xlabel_hour,tweet_nums_per_hour)\n",
    "plt.xlabel(\"time\",color=\"white\")\n",
    "plt.ylabel(\"sum_tweets\",color=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[35.6, 139.7], zoom_start=5.0)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "for index, r in time_line_df[(time_line_df['month']==10) & (time_line_df['day']==6)].iterrows():\n",
    "    folium.Marker([r.UserPlase_latitude,r.UserPlase_longitude]).add_to(marker_cluster)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SharedTask')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6feaf4af813783365efc585a6800850988678764fcb0bba12eca45fe4d31fdb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
